{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjm81D1Cgigp+g3cgbrcgw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivaShirsath/nlp/blob/master/3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wHcHeuIqPQ7i"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# load dataset\n",
        "!wget https://github.com/PICT-NLP/BE-NLP-Elective/raw/main/3-Preprocessing/News_dataset.pickle\n",
        "with open('News_dataset.pickle', 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "print(dataset)\n",
        "\n",
        "# convert dataset into pandas dataframe\n",
        "df = pd.DataFrame(dataset, columns=['text', 'category'])\n",
        "print(df.head())\n",
        "# convert text column to string type\n",
        "df['text'] = df['text'].astype(str)"
      ],
      "metadata": {
        "id": "ccxE4nQDs7F1",
        "outputId": "1401e325-a395-483b-b3e4-937523c6e817",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-08 14:52:58--  https://github.com/PICT-NLP/BE-NLP-Elective/raw/main/3-Preprocessing/News_dataset.pickle\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/PICT-NLP/BE-NLP-Elective/main/3-Preprocessing/News_dataset.pickle [following]\n",
            "--2023-03-08 14:53:00--  https://raw.githubusercontent.com/PICT-NLP/BE-NLP-Elective/main/3-Preprocessing/News_dataset.pickle\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4980629 (4.7M) [application/octet-stream]\n",
            "Saving to: ‘News_dataset.pickle’\n",
            "\n",
            "News_dataset.pickle 100%[===================>]   4.75M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-03-08 14:53:00 (299 MB/s) - ‘News_dataset.pickle’ saved [4980629/4980629]\n",
            "\n",
            "     File_Name                                            Content  Category  \\\n",
            "0      001.txt  Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...  business   \n",
            "1      002.txt  Dollar gains on Greenspan speech\\r\\n\\r\\nThe do...  business   \n",
            "2      003.txt  Yukos unit buyer faces loan claim\\r\\n\\r\\nThe o...  business   \n",
            "3      004.txt  High fuel prices hit BA's profits\\r\\n\\r\\nBriti...  business   \n",
            "4      005.txt  Pernod takeover talk lifts Domecq\\r\\n\\r\\nShare...  business   \n",
            "...        ...                                                ...       ...   \n",
            "2220   397.txt  BT program to beat dialler scams\\r\\n\\r\\nBT is ...      tech   \n",
            "2221   398.txt  Spam e-mails tempt net shoppers\\r\\n\\r\\nCompute...      tech   \n",
            "2222   399.txt  Be careful how you code\\r\\n\\r\\nA new European ...      tech   \n",
            "2223   400.txt  US cyber security chief resigns\\r\\n\\r\\nThe man...      tech   \n",
            "2224   401.txt  Losing yourself in online gaming\\r\\n\\r\\nOnline...      tech   \n",
            "\n",
            "     Complete_Filename  id  News_length  \n",
            "0     001.txt-business   1         2569  \n",
            "1     002.txt-business   1         2257  \n",
            "2     003.txt-business   1         1557  \n",
            "3     004.txt-business   1         2421  \n",
            "4     005.txt-business   1         1575  \n",
            "...                ...  ..          ...  \n",
            "2220      397.txt-tech   1         2526  \n",
            "2221      398.txt-tech   1         2294  \n",
            "2222      399.txt-tech   1         6297  \n",
            "2223      400.txt-tech   1         2323  \n",
            "2224      401.txt-tech   1        16248  \n",
            "\n",
            "[2225 rows x 6 columns]\n",
            "   text  category\n",
            "0   NaN       NaN\n",
            "1   NaN       NaN\n",
            "2   NaN       NaN\n",
            "3   NaN       NaN\n",
            "4   NaN       NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text cleaning\n",
        "df['text'] = df['text'].str.lower()\n",
        "df['text'] = df['text'].str.replace('[^\\w\\s]','')\n",
        "df['text'] = df['text'].str.replace('\\d+','')"
      ],
      "metadata": {
        "id": "5V49CC6qtDv7",
        "outputId": "e745e060-1267-4060-b362-759760e72bb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e136ade6f74d>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['text'] = df['text'].str.replace('[^\\w\\s]','')\n",
            "<ipython-input-3-e136ade6f74d>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['text'] = df['text'].str.replace('\\d+','')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['text'] = df['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))"
      ],
      "metadata": {
        "id": "2IPrG1smtIkT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stop word removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "print(df['text'])"
      ],
      "metadata": {
        "id": "N7nof3UutNJU",
        "outputId": "e153ef07-ac04-4dcc-dadf-6e61109a39bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       nan\n",
            "1       nan\n",
            "2       nan\n",
            "3       nan\n",
            "4       nan\n",
            "       ... \n",
            "2220    nan\n",
            "2221    nan\n",
            "2222    nan\n",
            "2223    nan\n",
            "2224    nan\n",
            "Name: text, Length: 2225, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# label encoding\n",
        "encoder = LabelEncoder()\n",
        "df['category'] = encoder.fit_transform(df['category'])\n",
        "print(df['category'])"
      ],
      "metadata": {
        "id": "0LTMmdDLtRo_",
        "outputId": "72ed16d5-c2c0-400d-a793-42f3031e7a70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       0\n",
            "1       0\n",
            "2       0\n",
            "3       0\n",
            "4       0\n",
            "       ..\n",
            "2220    0\n",
            "2221    0\n",
            "2222    0\n",
            "2223    0\n",
            "2224    0\n",
            "Name: category, Length: 2225, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF vectorization\n",
        "tfidf = TfidfVectorizer()\n",
        "X = tfidf.fit_transform(df['text'])\n",
        "print(X)"
      ],
      "metadata": {
        "id": "xeyu8LW9tVtC",
        "outputId": "a6c16c6a-0dc8-43eb-b376-03f87015d317",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t1.0\n",
            "  (1, 0)\t1.0\n",
            "  (2, 0)\t1.0\n",
            "  (3, 0)\t1.0\n",
            "  (4, 0)\t1.0\n",
            "  (5, 0)\t1.0\n",
            "  (6, 0)\t1.0\n",
            "  (7, 0)\t1.0\n",
            "  (8, 0)\t1.0\n",
            "  (9, 0)\t1.0\n",
            "  (10, 0)\t1.0\n",
            "  (11, 0)\t1.0\n",
            "  (12, 0)\t1.0\n",
            "  (13, 0)\t1.0\n",
            "  (14, 0)\t1.0\n",
            "  (15, 0)\t1.0\n",
            "  (16, 0)\t1.0\n",
            "  (17, 0)\t1.0\n",
            "  (18, 0)\t1.0\n",
            "  (19, 0)\t1.0\n",
            "  (20, 0)\t1.0\n",
            "  (21, 0)\t1.0\n",
            "  (22, 0)\t1.0\n",
            "  (23, 0)\t1.0\n",
            "  (24, 0)\t1.0\n",
            "  :\t:\n",
            "  (2200, 0)\t1.0\n",
            "  (2201, 0)\t1.0\n",
            "  (2202, 0)\t1.0\n",
            "  (2203, 0)\t1.0\n",
            "  (2204, 0)\t1.0\n",
            "  (2205, 0)\t1.0\n",
            "  (2206, 0)\t1.0\n",
            "  (2207, 0)\t1.0\n",
            "  (2208, 0)\t1.0\n",
            "  (2209, 0)\t1.0\n",
            "  (2210, 0)\t1.0\n",
            "  (2211, 0)\t1.0\n",
            "  (2212, 0)\t1.0\n",
            "  (2213, 0)\t1.0\n",
            "  (2214, 0)\t1.0\n",
            "  (2215, 0)\t1.0\n",
            "  (2216, 0)\t1.0\n",
            "  (2217, 0)\t1.0\n",
            "  (2218, 0)\t1.0\n",
            "  (2219, 0)\t1.0\n",
            "  (2220, 0)\t1.0\n",
            "  (2221, 0)\t1.0\n",
            "  (2222, 0)\t1.0\n",
            "  (2223, 0)\t1.0\n",
            "  (2224, 0)\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save outputs\n",
        "np.save('X.npy', X.toarray())\n",
        "np.save('y.npy', df['category'].to_numpy())"
      ],
      "metadata": {
        "id": "zXmCI1OztZ-S"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}